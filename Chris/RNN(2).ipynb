{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a504bd5-33f3-4135-b694-5d160efe0736",
   "metadata": {},
   "source": [
    "# Cuda setup\n",
    "Check if cuda is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0699a32e-8d74-4417-b8c7-43e55639f628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc568980-9012-4439-b9c4-bae733988f00",
   "metadata": {},
   "source": [
    "# Import data from source\n",
    "METHOD 1: install yfinance in the environment hosting python and jupyter. I used Anaconda, and installed through conda terminal into my environment.\n",
    "\n",
    "*pip install yfinance*\n",
    "\n",
    "Use the yfinance API to retrieve company data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa97f9a-30e8-4452-b67d-583a13662bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yahoo finance api to collect stock data\n",
    "import yfinance as yf\n",
    "import os\n",
    "\n",
    "# datetime imports to work with dates\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# alpaca api to collect stock data\n",
    "from alpaca_trade_api.rest import REST, TimeFrame, TimeFrameUnit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0af98-4596-4825-ba6b-da68f5872ca6",
   "metadata": {},
   "source": [
    "# Process data from csv files\n",
    "Use pandas library for processing files, and use matplotlib to display graphs and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7576d49-c91e-4e00-a572-9daca579a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c5609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chr1s\\Downloads\n",
      "AMD1_intraday.csv\n",
      "AMD_intraday.csv\n",
      "AMD_intraday1.csv\n",
      "AMD_intraday2.csv\n",
      "AMD_intraday3.csv\n",
      "build_cnn.ipynb\n",
      "CSVsetup.ipynb\n",
      "debug.log\n",
      "desktop.ini\n",
      "first_try.ipynb\n",
      "ideaIU-2024.3.3.exe\n",
      "MariaResearch.sql\n",
      "model_trained.pt\n",
      "openjfx-24.0.1_windows-x64_bin-sdk.zip\n",
      "RNN(2).ipynb\n",
      "RNN.ipynb\n",
      "ubuntu-24.04.2-wsl-amd64.gz\n",
      "untitled.txt\n",
      "WebStorm-2024.3.3.exe\n",
      "[WinError 3] The system cannot find the path specified: '/mnt/c/Users/LPC/Documents/GitHub/CSCI611-NNSTOCKS'\n",
      "C:\\Users\\chr1s\\Downloads\n",
      "C:\\Users\\chr1s\\Downloads\n",
      "AMD1_intraday.csv\n",
      "AMD_intraday.csv\n",
      "AMD_intraday1.csv\n",
      "AMD_intraday2.csv\n",
      "AMD_intraday3.csv\n",
      "build_cnn.ipynb\n",
      "CSVsetup.ipynb\n",
      "debug.log\n",
      "desktop.ini\n",
      "first_try.ipynb\n",
      "ideaIU-2024.3.3.exe\n",
      "MariaResearch.sql\n",
      "model_trained.pt\n",
      "openjfx-24.0.1_windows-x64_bin-sdk.zip\n",
      "RNN(2).ipynb\n",
      "RNN.ipynb\n",
      "ubuntu-24.04.2-wsl-amd64.gz\n",
      "untitled.txt\n",
      "WebStorm-2024.3.3.exe\n"
     ]
    }
   ],
   "source": [
    "#Change directory for Lorne's jupyter notebook\n",
    "# I am mixing windows and wsl on windows so I need to manaually change the directory, so you won't need to when you run it\n",
    "if True:\n",
    "    # See files in current directory\n",
    "    import os\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    print(current_directory)\n",
    "\n",
    "    entries = os.listdir('.')\n",
    "    files = [entry for entry in entries if os.path.isfile(entry)]\n",
    "\n",
    "    for file_name in files:\n",
    "        print(file_name)\n",
    "\n",
    "    %cd \"/mnt/c/Users/LPC/Documents/GitHub/CSCI611-NNSTOCKS\"\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    print(current_directory)\n",
    "\n",
    "    entries = os.listdir('.')\n",
    "    files = [entry for entry in entries if os.path.isfile(entry)]\n",
    "\n",
    "    # Print the names of the files\n",
    "    for file_name in files:\n",
    "        print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f264cc-de65-4935-ac22-8300a6df8f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_percentage = 6\n",
    "valid_percentage = 2\n",
    "testing_percentage = 2\n",
    "class StockDataset(torch.utils.data.Dataset[float]):\n",
    "    def __init__(self, sequences, targets):\n",
    "        super(StockDataset).__init__()\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)    \n",
    "    def __getitem__(self, index):\n",
    "        sequence = torch.tensor(self.sequences[index], dtype=torch.float32).unsqueeze(-1)#sequence at index \n",
    "        target = torch.tensor(self.targets[index], dtype=torch.float32)#test_value at index\n",
    "        return sequence, target\n",
    "        \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn1 = nn.RNN(input_size=1, hidden_size=128, num_layers=2, nonlinearity='tanh', bias=True, batch_first=True, dropout=0.0, bidirectional=False, device=None, dtype=None)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.output, self.hidden = self.rnn1(x)\n",
    "        prediction = self.fc(self.output[:, -1, :])\n",
    "        return prediction.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef45ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate stock price data from yahoo finance \n",
    "def get_yahoo_stock_data(name, interval=\"5m\", period=\"7d\"):\n",
    "    data = yf.download(name, interval=interval, period=period)\n",
    "    return data\n",
    "\n",
    "def get_alpaca_stock_data(name, interval=\"15\", months=\"6\"):\n",
    "    name_of_file = name + \"_intraday1.csv\"\n",
    "\n",
    "    start_date = date(2021, 6, 1)\n",
    "    end_date = start_date + relativedelta(months=int(months))  # Adds months\n",
    "\n",
    "    api = REST('PKJ41QP5QU0TYS4S1BYB', 'o5HVFGx0XWSMoMyeQdRJwG1apYXtuMNcguWpjqqe')\n",
    "\n",
    "    data = api.get_bars(name, TimeFrame(int(interval), TimeFrameUnit.Minute), start_date, end_date, adjustment='raw').df\n",
    "\n",
    "    data = data.rename(columns={\"close\": \"Close\", \"open\": \"Price\", \"high\": \"High\", \"low\": \"Low\", \"volume\": \"Volume\", \"datetime\": \"Datetime\", \"ticker\": \"Ticker\"})\n",
    "\n",
    "    data.to_csv(name_of_file)\n",
    "\n",
    "    return name_of_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f3c7f-aee6-4d8b-961a-bb7c073dd811",
   "metadata": {},
   "source": [
    "# Display relevant information for formatting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1056a69-21ac-4d87-8daa-760ac05dfc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_initializer:\n",
    "    #\n",
    "    def __init__(self, retrieve, name, recomp, nval, ival, pval, batch_size, num_workers, epochs, learning_rate, lr_scheduler_rate, beta1, beta2):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_scheduler_rate = lr_scheduler_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        if(retrieve == True):\n",
    "            if (stock_data_source == \"yahoo\"):\n",
    "                self.csv_name = self.retrieve_csv(name, recomp, nval, str(ival[0])+ival[1], str(pval[0])+pval[1])\n",
    "            else:\n",
    "                self.csv_name = get_alpaca_stock_data(name, stock_interval, stock_period)\n",
    "        else:\n",
    "            self.csv_name = name\n",
    "        self.df=pd.read_csv(name + \"_intraday1.csv\")\n",
    "        self.show_df_info()\n",
    "        #format data, and prepare it for RNN\n",
    "        if(stock_data_source == \"yahoo\"):\n",
    "            price = self.df['Close'].to_list()[2:]\n",
    "            self.axis_labels = self.df['Price'].to_list()[2:]\n",
    "        else:\n",
    "            price = self.df['Close'].to_list()[1:]\n",
    "            self.axis_labels = self.df['Price'].to_list()[1:]\n",
    "        date_format_with_time = \"%Y-%m-%d %H:%M:%S\"\n",
    "        self.price_inputs = [float(x) for x in price]\n",
    "        sequence_length = 4\n",
    "        #Training sets\n",
    "        self.train_seq = []\n",
    "        self.train_tar = []\n",
    "        #Validation sets\n",
    "        self.valid_seq = []\n",
    "        self.valid_tar = []\n",
    "        #Testing sets\n",
    "        self.test_seq = []\n",
    "        self.test_tar = []\n",
    "        #choose a selected time range\n",
    "        #NOTE\n",
    "        train_range = (len(self.price_inputs)//10 * training_percentage)\n",
    "        print(len(self.price_inputs))\n",
    "        print(train_range)\n",
    "        valid_range_beg = train_range\n",
    "        valid_range_end = train_range + (len(self.price_inputs)//10 * valid_percentage)\n",
    "        print(valid_range_end)\n",
    "        test_range_beg = valid_range_end\n",
    "        test_range_end = valid_range_end + (len(self.price_inputs)//10 * testing_percentage)\n",
    "        print(test_range_end)\n",
    "        #generate sequences and targets list for loading data\n",
    "        for i in range(train_range - sequence_length):\n",
    "            seq = self.price_inputs[i:i+sequence_length]\n",
    "            self.train_seq.append(seq)\n",
    "            temp = self.price_inputs[i+sequence_length]\n",
    "            self.train_tar.append(temp)\n",
    "        for j in range(valid_range_beg, valid_range_end-sequence_length):\n",
    "            seq = self.price_inputs[j:j+sequence_length]\n",
    "            self.valid_seq.append(seq)\n",
    "            temp = self.price_inputs[j+sequence_length]\n",
    "            self.valid_tar.append(temp)\n",
    "        for k in range(test_range_beg, test_range_end-sequence_length):\n",
    "            seq = self.price_inputs[k:k+sequence_length]\n",
    "            self.test_seq.append(seq)\n",
    "            temp = self.price_inputs[k+sequence_length]\n",
    "            self.test_tar.append(temp)\n",
    "        train_data = StockDataset(self.train_seq, self.train_tar)\n",
    "        valid_data = StockDataset(self.valid_seq, self.valid_tar)\n",
    "        test_data = StockDataset(self.test_seq, self.test_tar)\n",
    "        \n",
    "        self.train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "        self.valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, num_workers=num_workers)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n",
    "    # retreive_csv is a file that allows the user to extract stock data from yahoo finance.\n",
    "    # @param: name, name of file\n",
    "    # @param: recomp, indicates if file needs to be recompiled\n",
    "    # @param: num, indicates which file need to be recompiled\n",
    "    def retrieve_csv(self, name, recomp, nval, ival, pval):\n",
    "        # Example: Get 1-minute intraday data for Apple (AAPL) for 1 day\n",
    "        data = get_yahoo_stock_data(stock_name, stock_interval, stock_period)\n",
    "        ext = \".csv\"\n",
    "        pt2 = \"_intraday\"\n",
    "        num = 1;\n",
    "        file_name = name + pt2 + str(num) + ext\n",
    "        found = False\n",
    "        if(recomp!=True):\n",
    "            while(found!=True):\n",
    "                if os.path.isfile(file_name):\n",
    "                    num+=1\n",
    "                    file_name = name + pt2 + str(num) + ext\n",
    "                else:\n",
    "                    found = True\n",
    "        else:\n",
    "            if(nval >= 1):\n",
    "                file_name = name + pt2 + str(nval) + ext\n",
    "            else:\n",
    "                file_name = name + pt2 + ext\n",
    "        data.to_csv(file_name)\n",
    "        return file_name\n",
    "\n",
    "    def display_fig(self):\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.title(name + \" Intraday Stock Price\")\n",
    "        plt.plot(self.axis_labels, self.price_inputs)\n",
    "        plt.xlabel(\"time\")\n",
    "        plt.ylabel(\"price\")\n",
    "        plt.xticks(self.axis_labels[::26])\n",
    "        plt.yticks(self.price_inputs[::30])\n",
    "        plt.show()\n",
    "\n",
    "    def show_df_info(self):\n",
    "        self.df.head(15)\n",
    "        self.df.tail(10)\n",
    "        print(\"Row count: \" + str(len(self.df)))\n",
    "        \n",
    "    def trainAndTest(self):\n",
    "        #RNN model\n",
    "        self.rnn1 = RNN()\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        min_loss = np.inf\n",
    "\n",
    "        if train_on_gpu:\n",
    "            self.rnn1.cuda()\n",
    "        #use MSELoss instead of MSEAbsoluteLoss (predicting next price compared to next change)\n",
    "        error = nn.MSELoss()\n",
    "        # specify optimizer\n",
    "        optimizer = torch.optim.Adam(self.rnn1.parameters(), lr=self.learning_rate, betas=(beta1, beta2))\n",
    "        #optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "        #self.price_tensor = torch.tensor(self.sequences, dtype=torch.float32).unsqueeze(-1)#input \n",
    "        #self.y_tensor = torch.tensor(self.test_vals, dtype=torch.float32)#test_values\n",
    "        # Learning rate scheduler\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=self.lr_scheduler_rate, patience=10)\n",
    "        valid_loss_min = np.inf\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "            #TRAINING\n",
    "            self.rnn1.train()\n",
    "            for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "                # move tensors to GPU if CUDA is available\n",
    "                if train_on_gpu:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                # clear the gradients of all optimized variables\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = self.rnn1(data)\n",
    "                # calculate the batch loss\n",
    "                loss_train = error(output, target)\n",
    "                # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                loss_train.backward()\n",
    "                # perform a single optimization step (parameter update)\n",
    "                optimizer.step()\n",
    "                train_loss += loss_train.item()*data.size(0)\n",
    "                if (loss_train < min_loss):\n",
    "                    min_loss = loss_train\n",
    "                    #torch.save(self.rnn1.state_dict(), \"rnn1.pth\")\n",
    "\n",
    "            scheduler.step(train_loss)  # Update learning rate\n",
    "\n",
    "            \n",
    "            if (epoch+1) % 10 == 0:\n",
    "                lr = optimizer.param_groups[0][\"lr\"]\n",
    "                print(f\"Training: Epoch {epoch+1}/{epochs}, Loss: {loss_train.item():.6f}\")\n",
    "                #print(f\"Validation: Epoch {epoch+1}/{epochs}, Loss: {loss_valid.item():.6f}\")\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        self.rnn1.eval()\n",
    "        for batch_idx, (data, target) in enumerate(self.valid_loader):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = self.rnn1(data)\n",
    "            # calculate the batch loss\n",
    "            loss_valid = error(output, target)\n",
    "            # perform a single optimization step (parameter update)\n",
    "            valid_loss += loss_valid.item()*data.size(0)\n",
    "            if (loss_valid < min_loss):\n",
    "                min_loss = loss_valid\n",
    "                #torch.save(self.rnn1.state_dict(), \"rnn1.pth\")\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(self.train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(self.valid_loader.dataset)\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(self.rnn1.state_dict(), 'model_trained.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "        return min_loss\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "962b09fa-139f-4317-a936-9d306848fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stock variables used when collecting stock data\n",
    "stock_data_source = \"alpaca\"\n",
    "#stock_data_source = \"yahoo\"\n",
    "if stock_data_source == \"yahoo\":\n",
    "    #Stock variables when using yahoo finance api\n",
    "    stock_name = \"AMD\"\n",
    "    stock_interval=\"5m\"\n",
    "    stock_period=\"7d\"\n",
    "    epochs = 100\n",
    "    lr_scheduler_rate = 0.8\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "\n",
    "#Stock variables when using alpaca api\n",
    "if stock_data_source == \"alpaca\":\n",
    "    stock_name = \"AMD\"\n",
    "    stock_interval=\"15\"\n",
    "    stock_period=\"4\" #months\n",
    "    epochs = 100\n",
    "    lr_scheduler_rate = 0.5\n",
    "    beta1 = 0.95\n",
    "    beta2 = 0.999\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c40c65e-3a8a-4306-bb71-a365246c813d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.005    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 0.597513\n",
      "Training: Epoch 20/100, Loss: 30.138077\n",
      "Training: Epoch 30/100, Loss: 119.862183\n",
      "Training: Epoch 40/100, Loss: 179.323654\n",
      "Training: Epoch 50/100, Loss: 178.067429\n",
      "Training: Epoch 60/100, Loss: 207.302460\n",
      "Training: Epoch 70/100, Loss: 213.889252\n",
      "Training: Epoch 80/100, Loss: 220.733643\n",
      "Training: Epoch 90/100, Loss: 220.855164\n",
      "Training: Epoch 100/100, Loss: 226.120636\n",
      "Validation loss decreased (inf --> 276.336864).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.001    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 191.614990\n",
      "Training: Epoch 20/100, Loss: 191.580170\n",
      "Training: Epoch 30/100, Loss: 199.752808\n",
      "Training: Epoch 40/100, Loss: 212.844772\n",
      "Training: Epoch 50/100, Loss: 222.943939\n",
      "Training: Epoch 60/100, Loss: 197.183517\n",
      "Training: Epoch 70/100, Loss: 95.394470\n",
      "Training: Epoch 80/100, Loss: 26.648077\n",
      "Training: Epoch 90/100, Loss: 3.154637\n",
      "Training: Epoch 100/100, Loss: 0.089882\n",
      "Validation loss decreased (inf --> 5.243598).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.0005    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 479.331726\n",
      "Training: Epoch 20/100, Loss: 204.277695\n",
      "Training: Epoch 30/100, Loss: 212.985687\n",
      "Training: Epoch 40/100, Loss: 220.167404\n",
      "Training: Epoch 50/100, Loss: 224.811386\n",
      "Training: Epoch 60/100, Loss: 168.165390\n",
      "Training: Epoch 70/100, Loss: 69.465363\n",
      "Training: Epoch 80/100, Loss: 16.800041\n",
      "Training: Epoch 90/100, Loss: 0.878261\n",
      "Training: Epoch 100/100, Loss: 0.809610\n",
      "Validation loss decreased (inf --> 4.357261).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.0001    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 5914.030273\n",
      "Training: Epoch 20/100, Loss: 3100.918945\n",
      "Training: Epoch 30/100, Loss: 1386.788818\n",
      "Training: Epoch 40/100, Loss: 543.495728\n",
      "Training: Epoch 50/100, Loss: 275.294006\n",
      "Training: Epoch 60/100, Loss: 230.641846\n",
      "Training: Epoch 70/100, Loss: 228.085922\n",
      "Training: Epoch 80/100, Loss: 228.976501\n",
      "Training: Epoch 90/100, Loss: 159.157074\n",
      "Training: Epoch 100/100, Loss: 76.838181\n",
      "Validation loss decreased (inf --> 109.190512).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  5e-05    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 7831.114746\n",
      "Training: Epoch 20/100, Loss: 5877.357422\n",
      "Training: Epoch 30/100, Loss: 4256.083496\n",
      "Training: Epoch 40/100, Loss: 2935.816406\n",
      "Training: Epoch 50/100, Loss: 1904.369507\n",
      "Training: Epoch 60/100, Loss: 1150.509766\n",
      "Training: Epoch 70/100, Loss: 656.919312\n",
      "Training: Epoch 80/100, Loss: 389.330414\n",
      "Training: Epoch 90/100, Loss: 279.551941\n",
      "Training: Epoch 100/100, Loss: 244.262268\n",
      "Validation loss decreased (inf --> 296.324625).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  1e-05    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 9737.358398\n",
      "Training: Epoch 20/100, Loss: 9221.232422\n",
      "Training: Epoch 30/100, Loss: 8742.711914\n",
      "Training: Epoch 40/100, Loss: 8281.458984\n",
      "Training: Epoch 50/100, Loss: 7834.207031\n",
      "Training: Epoch 60/100, Loss: 7400.181152\n",
      "Training: Epoch 70/100, Loss: 6979.152344\n",
      "Training: Epoch 80/100, Loss: 6571.063477\n",
      "Training: Epoch 90/100, Loss: 6175.881836\n",
      "Training: Epoch 100/100, Loss: 5793.597168\n",
      "Validation loss decreased (inf --> 6018.770619).  Saving model ...\n",
      "\n",
      "\n",
      "Best learning rate:  0.0005    Loss:  tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Second best learning rate:  0.005    Loss:  tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.0005    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 491.784637\n",
      "Training: Epoch 20/100, Loss: 204.576965\n",
      "Training: Epoch 30/100, Loss: 213.020859\n",
      "Training: Epoch 40/100, Loss: 220.179367\n",
      "Training: Epoch 50/100, Loss: 224.839722\n",
      "Training: Epoch 60/100, Loss: 169.345947\n",
      "Training: Epoch 70/100, Loss: 68.343079\n",
      "Training: Epoch 80/100, Loss: 16.439753\n",
      "Training: Epoch 90/100, Loss: 0.754417\n",
      "Training: Epoch 100/100, Loss: 0.791181\n",
      "Validation loss decreased (inf --> 4.912704).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.0009500000000000001    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 196.411835\n",
      "Training: Epoch 20/100, Loss: 182.963654\n",
      "Training: Epoch 30/100, Loss: 193.411057\n",
      "Training: Epoch 40/100, Loss: 213.917603\n",
      "Training: Epoch 50/100, Loss: 223.217133\n",
      "Training: Epoch 60/100, Loss: 223.653168\n",
      "Training: Epoch 70/100, Loss: 225.252472\n",
      "Training: Epoch 80/100, Loss: 132.048248\n",
      "Training: Epoch 90/100, Loss: 46.897991\n",
      "Training: Epoch 100/100, Loss: 8.648439\n",
      "Validation loss decreased (inf --> 23.933566).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.0014000000000000002    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 158.762772\n",
      "Training: Epoch 20/100, Loss: 178.108978\n",
      "Training: Epoch 30/100, Loss: 201.974213\n",
      "Training: Epoch 40/100, Loss: 212.150238\n",
      "Training: Epoch 50/100, Loss: 220.062775\n",
      "Training: Epoch 60/100, Loss: 225.847504\n",
      "Training: Epoch 70/100, Loss: 198.134674\n",
      "Training: Epoch 80/100, Loss: 110.870628\n",
      "Training: Epoch 90/100, Loss: 47.764656\n",
      "Training: Epoch 100/100, Loss: 13.170692\n",
      "Validation loss decreased (inf --> 30.561135).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.00185    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 117.701599\n",
      "Training: Epoch 20/100, Loss: 155.934875\n",
      "Training: Epoch 30/100, Loss: 194.227707\n",
      "Training: Epoch 40/100, Loss: 210.392197\n",
      "Training: Epoch 50/100, Loss: 218.154770\n",
      "Training: Epoch 60/100, Loss: 224.162033\n",
      "Training: Epoch 70/100, Loss: 227.726776\n",
      "Training: Epoch 80/100, Loss: 195.927460\n",
      "Training: Epoch 90/100, Loss: 124.669418\n",
      "Training: Epoch 100/100, Loss: 72.489738\n",
      "Validation loss decreased (inf --> 103.901762).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.0023    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 79.982521\n",
      "Training: Epoch 20/100, Loss: 133.768890\n",
      "Training: Epoch 30/100, Loss: 184.219406\n",
      "Training: Epoch 40/100, Loss: 208.075073\n",
      "Training: Epoch 50/100, Loss: 217.206451\n",
      "Training: Epoch 60/100, Loss: 223.145355\n",
      "Training: Epoch 70/100, Loss: 227.039459\n",
      "Training: Epoch 80/100, Loss: 229.302063\n",
      "Training: Epoch 90/100, Loss: 214.750366\n",
      "Training: Epoch 100/100, Loss: 163.178314\n",
      "Validation loss decreased (inf --> 206.638834).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.0027500000000000003    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 47.041576\n",
      "Training: Epoch 20/100, Loss: 112.273186\n",
      "Training: Epoch 30/100, Loss: 173.754684\n",
      "Training: Epoch 40/100, Loss: 203.731613\n",
      "Training: Epoch 50/100, Loss: 214.902756\n",
      "Training: Epoch 60/100, Loss: 221.503174\n",
      "Training: Epoch 70/100, Loss: 226.087830\n",
      "Training: Epoch 80/100, Loss: 193.146042\n",
      "Training: Epoch 90/100, Loss: 106.213135\n",
      "Training: Epoch 100/100, Loss: 45.339691\n",
      "Validation loss decreased (inf --> 71.653241).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.0032    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 24.223003\n",
      "Training: Epoch 20/100, Loss: 91.539360\n",
      "Training: Epoch 30/100, Loss: 162.767609\n",
      "Training: Epoch 40/100, Loss: 199.586166\n",
      "Training: Epoch 50/100, Loss: 214.489227\n",
      "Training: Epoch 60/100, Loss: 221.124817\n",
      "Training: Epoch 70/100, Loss: 223.076019\n",
      "Training: Epoch 80/100, Loss: 140.753845\n",
      "Training: Epoch 90/100, Loss: 61.279766\n",
      "Training: Epoch 100/100, Loss: 16.686918\n",
      "Validation loss decreased (inf --> 35.293686).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.0036500000000000005    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 10.180561\n",
      "Training: Epoch 20/100, Loss: 72.885307\n",
      "Training: Epoch 30/100, Loss: 152.001266\n",
      "Training: Epoch 40/100, Loss: 194.667419\n",
      "Training: Epoch 50/100, Loss: 212.449432\n",
      "Training: Epoch 60/100, Loss: 219.814575\n",
      "Training: Epoch 70/100, Loss: 224.893555\n",
      "Training: Epoch 80/100, Loss: 227.976959\n",
      "Training: Epoch 90/100, Loss: 229.697021\n",
      "Training: Epoch 100/100, Loss: 230.475952\n",
      "Validation loss decreased (inf --> 281.494996).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.0041    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 2.866379\n",
      "Training: Epoch 20/100, Loss: 56.237755\n",
      "Training: Epoch 30/100, Loss: 141.216309\n",
      "Training: Epoch 40/100, Loss: 189.634201\n",
      "Training: Epoch 50/100, Loss: 210.389984\n",
      "Training: Epoch 60/100, Loss: 218.549286\n",
      "Training: Epoch 70/100, Loss: 224.067017\n",
      "Training: Epoch 80/100, Loss: 227.536499\n",
      "Training: Epoch 90/100, Loss: 229.485504\n",
      "Training: Epoch 100/100, Loss: 230.368042\n",
      "Validation loss decreased (inf --> 281.366030).  Saving model ...\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Learning rate:  0.00455    Loop:  1\n",
      "AMD Time interval  15 Time period:  4\n",
      "Batch size:  16 Number of workers:  0 Epochs:  100\n",
      "Learning rate scheduler rate:  0.5\n",
      "Beta1:  0.95 Beta2:  0.999\n",
      "-------------------------------------------------------------------------------\n",
      "Row count: 5273\n",
      "5272\n",
      "3162\n",
      "4216\n",
      "5270\n",
      "Training: Epoch 10/100, Loss: 0.229587\n",
      "Training: Epoch 20/100, Loss: 41.957542\n",
      "Training: Epoch 30/100, Loss: 130.481216\n",
      "Training: Epoch 40/100, Loss: 184.512512\n",
      "Training: Epoch 50/100, Loss: 208.296509\n",
      "Training: Epoch 60/100, Loss: 217.338913\n",
      "Training: Epoch 70/100, Loss: 223.238800\n",
      "Training: Epoch 80/100, Loss: 227.091415\n",
      "Training: Epoch 90/100, Loss: 230.043091\n",
      "Training: Epoch 100/100, Loss: 213.445526\n",
      "Validation loss decreased (inf --> 262.231541).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "#test various learning rates\n",
    "learning_rate_list = [0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "best_lr = [[np.inf, np.inf] , [np.inf, np.inf]] #record two pairs of [loss, learning rate] to tune learning rate later\n",
    "best_lr_in_loop = np.inf\n",
    "repeated_loops_per_lr = 1\n",
    "rnnControl = RNN_initializer(retrieve=True, name=stock_name, recomp=True, nval=1, ival=[5, \"m\"], pval=[7,\"d\"], \n",
    "                             batch_size=batch_size, num_workers=num_workers, epochs=epochs, learning_rate=0.001, lr_scheduler_rate=lr_scheduler_rate,\n",
    "                             beta1=beta1, beta2=beta2)\n",
    "bestRNN = [rnnControl,None]\n",
    "bestRNN_in_loop = rnnControl\n",
    "for lr in learning_rate_list:\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------\")\n",
    "    print(\"Learning rate: \", lr, \"   Loop: \", repeated_loops_per_lr)\n",
    "    print(stock_name, \"Time interval \", stock_interval, \"Time period: \", stock_period)\n",
    "    print(\"Batch size: \", batch_size, \"Number of workers: \", num_workers, \"Epochs: \", epochs)\n",
    "    print(\"Learning rate scheduler rate: \", lr_scheduler_rate)\n",
    "    print(\"Beta1: \", beta1, \"Beta2: \", beta2)\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    \n",
    "    for i in range(repeated_loops_per_lr):\n",
    "        rnn1 = RNN_initializer(retrieve=False, name=stock_name, recomp=True, nval=1, ival=[5, \"m\"], pval=[7,\"d\"], batch_size=batch_size, num_workers=num_workers, epochs=epochs, learning_rate=lr, lr_scheduler_rate=lr_scheduler_rate, beta1=beta1, beta2=beta2)\n",
    "        #returns\n",
    "        loss = rnn1.trainAndTest()\n",
    "        if best_lr_in_loop > loss:\n",
    "            best_lr_in_loop = loss\n",
    "            bestRNN_in_loop = rnn1\n",
    "    #if found best lr, remove worst lr from list\n",
    "    if best_lr_in_loop < best_lr[0][0]:\n",
    "        best_lr[1] = best_lr[0]\n",
    "        best_lr[0] = [best_lr_in_loop, lr]\n",
    "        bestRNN[1] = bestRNN[0]\n",
    "        bestRNN[0] = bestRNN_in_loop\n",
    "        #if \n",
    "    elif best_lr_in_loop < best_lr[1][0]:\n",
    "        best_lr[1] = [best_lr_in_loop, lr]\n",
    "        bestRNN[1] = bestRNN_in_loop\n",
    "    best_lr_in_loop = np.inf\n",
    "\n",
    "print(\"\\n\\nBest learning rate: \", best_lr[0][1], \"   Loss: \", best_lr[0][0])\n",
    "print(\"Second best learning rate: \", best_lr[1][1], \"   Loss: \", best_lr[1][0])\n",
    "\n",
    "\n",
    "learning_rate_list = []\n",
    "difference_of_lr = best_lr[0][1] - best_lr[1][1]\n",
    "number_of_increments = 10\n",
    "increment = difference_of_lr / number_of_increments\n",
    "\n",
    "for i in range(number_of_increments):\n",
    "    learning_rate_list.append(best_lr[0][1] - increment * i)\n",
    "\n",
    "for lr in learning_rate_list:\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------\")\n",
    "    print(\"Learning rate: \", lr, \"   Loop: \", repeated_loops_per_lr)\n",
    "    print(stock_name, \"Time interval \", stock_interval, \"Time period: \", stock_period)\n",
    "    print(\"Batch size: \", batch_size, \"Number of workers: \", num_workers, \"Epochs: \", epochs)\n",
    "    print(\"Learning rate scheduler rate: \", lr_scheduler_rate)\n",
    "    print(\"Beta1: \", beta1, \"Beta2: \", beta2)\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    \n",
    "    for i in range(repeated_loops_per_lr):\n",
    "        rnn1 = RNN_initializer(retrieve=False, name=stock_name, recomp=False, nval=1, ival=[5, \"m\"], pval=[7,\"d\"], batch_size=batch_size, num_workers=num_workers, epochs=epochs, learning_rate=lr, lr_scheduler_rate=lr_scheduler_rate, beta1=beta1, beta2=beta2)\n",
    "        loss = rnn1.trainAndTest()\n",
    "        if best_lr_in_loop > loss:\n",
    "            best_lr_in_loop = loss\n",
    "\n",
    "    if best_lr_in_loop < best_lr[0][0]:\n",
    "        best_lr[1] = best_lr[0]\n",
    "        best_lr[0] = [best_lr_in_loop, lr]\n",
    "    elif best_lr_in_loop < best_lr[1][0]:\n",
    "        best_lr[1] = [best_lr_in_loop, lr]\n",
    "\n",
    "    best_lr_in_loop = np.inf\n",
    "\n",
    "\n",
    "    #Make testing only, no training\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d87229d-268e-4d1b-b0d0-4ac7c128fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output,hidden = rnn1(price_tensor)\n",
    "#print(output.shape)  # (1, 1, 128)\n",
    "#print(hidden.shape)  # (2, 1, 128)\n",
    "  # Predict 1 value from hidden_size=128\n",
    "\n",
    "#prediction = fc(output[:, -1, :])  # Take output at last time step\n",
    "#print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74fb33-cc5a-47ca-9e9c-d413adbdfaa9",
   "metadata": {},
   "source": [
    "## Test the Trained Network\n",
    "---\n",
    "Test your trained model on previously unseen data! Remember we have downloaded `train_data` and `test_data`. We will use `test_data` through `test_loader`.\n",
    "\n",
    "A \"good\" result will be a CNN that gets around 70% (or more, try your best!) accuracy on these test images.\n",
    "\n",
    "The following is working code, but you are encouraged to make your own adjustments and enhance the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d43a4-8766-4323-bf5c-98ac8df970c4",
   "metadata": {},
   "source": [
    "### Specify [Loss Function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [Optimizer](http://pytorch.org/docs/stable/optim.html)\n",
    "---\n",
    "Decide on a loss and optimization function that is best suited for this classification task. The linked code examples from above, may be a good starting point; [this PyTorch classification example](https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py) Pay close attention to the value for **learning rate** as this value determines how your model converges to a small error.\n",
    "\n",
    "The following is working code, but you can make your own adjustments.\n",
    "\n",
    "**TODO**: try to compare with ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88d78c82-f8ad-4dd7-9105-5f0b04dcc77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error = nn.MSELoss()\n",
    "#optimizer = torch.optim.Adam(prediction.parameters(), lr=0.001)\n",
    "\n",
    "#epochs = 50\n",
    "#for epoch in range(epochs):\n",
    "#    rnn1.train()\n",
    "#    fc.train()\n",
    "    \n",
    "#      output,hidden = rnn1(price_tensor)\n",
    "#    prediction = fc(output[:, -1, :])\n",
    "#    loss = error(prediction, y_tensor)\n",
    "    \n",
    "#    optimizer.zero_grad()\n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n",
    "\n",
    "#    if (epoch+1) % 10 == 0:\n",
    "#        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8368bc5b-0ca2-4673-9395-87c52de5532e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 7.171965\n",
      "\n",
      "Test Accuracy:  7% (66/1050)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "value_correct = 0\n",
    "error = nn.MSELoss()\n",
    "\n",
    "bestRNN[0].rnn1.eval()\n",
    "# iterate over test data\n",
    "for batch_idx, (data, target) in enumerate(bestRNN[0].test_loader):\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = bestRNN[0].rnn1(data)\n",
    "    # calculate the batch loss\n",
    "    loss = error(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = output.eq(target.data.view_as(output))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    value_correct += 1\n",
    "        \n",
    "# average test loss\n",
    "test_loss = test_loss/len(bestRNN[0].test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "print('Test Accuracy: %2d%% (%2d/%2d)' % (test_loss,\n",
    "    value_correct, len(bestRNN[0].test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "691e06a1-95d1-4558-8c3e-0cd8d70a765d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RNN_initializer' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Define your models first\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#rnn1 = ...  # Your RNN model definition\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#fc = ...    # Your fully connected layer definition\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Define loss and optimizer\u001b[39;00m\n\u001b[32m      6\u001b[39m error = nn.MSELoss()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m optimizer = torch.optim.Adam(\u001b[38;5;28mlist\u001b[39m(rnn1.parameters()) + \u001b[38;5;28mlist\u001b[39m(fc.parameters()), \n\u001b[32m      8\u001b[39m                             lr=\u001b[32m0.001\u001b[39m, weight_decay=\u001b[32m0.001\u001b[39m)  \u001b[38;5;66;03m# L2 regularization\u001b[39;00m\n\u001b[32m      9\u001b[39m scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m3\u001b[39m, factor=\u001b[32m0.5\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Add gradient clipping value\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'RNN_initializer' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "# Define your models first\n",
    "#rnn1 = ...  # Your RNN model definition\n",
    "#fc = ...    # Your fully connected layer definition\n",
    "\n",
    "# Define loss and optimizer\n",
    "error = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(list(rnn1.parameters()) + list(fc.parameters()), \n",
    "                            lr=0.001, weight_decay=0.001)  # L2 regularization\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# Add gradient clipping value\n",
    "clip_value = 1.0\n",
    "\n",
    "# Add validation set monitoring\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "no_improvement = 0\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    rnn1.train()\n",
    "    fc.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    output, hidden = rnn1(price_tensor)\n",
    "    prediction = fc(output[:, -1, :])\n",
    "    loss = error(prediction, y_tensor)\n",
    "    \n",
    "    # Backward pass with gradient clipping\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(list(rnn1.parameters()) + list(fc.parameters()), clip_value)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(loss)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if loss.item() < best_val_loss:\n",
    "        best_val_loss = loss.item()\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        if no_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9311fa-b706-4266-91f8-c531673109cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
